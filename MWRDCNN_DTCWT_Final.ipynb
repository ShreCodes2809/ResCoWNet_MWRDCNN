{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4iaExAP7LZm"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "\n",
        "  Args:\n",
        "    dir_path (str): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow"
      ],
      "metadata": {
        "id": "mQcTH82u7U-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install keras"
      ],
      "metadata": {
        "id": "DXiOTfbB7U4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import os\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Lambda,Subtract,ZeroPadding2D,Dense, Add\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from  matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "vAffay3b7SYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import os\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import cv2\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Lambda,Subtract,ZeroPadding2D,Dense, Add\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from  matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "Vilptuf87XcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test and validation directories\n",
        "raw_train_dir = \"D:/SHREYASH_CODE/Raw_OCT_Train\"\n",
        "clean_train_dir = \"D:/SHREYASH_CODE/Clean_OCT_Train\"\n",
        "raw_val_dir = \"D:/SHREYASH_CODE/RawOCT_Val\"\n",
        "clean_val_dir = \"D:/SHREYASH_CODE/CleanOCT_Val\"\n",
        "raw_test_dir = \"D:/SHREYASH_CODE/RawOCT_Test\"\n",
        "clean_test_dir = \"D:/SHREYASH_CODE/CleanOCT_Test\""
      ],
      "metadata": {
        "id": "dEudHOa57ZFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(raw_train_dir)\n",
        "walk_through_dir(clean_train_dir)\n",
        "walk_through_dir(raw_val_dir)\n",
        "walk_through_dir(clean_val_dir)\n",
        "walk_through_dir(raw_test_dir)\n",
        "walk_through_dir(clean_test_dir)"
      ],
      "metadata": {
        "id": "KVbGDVqJ7apr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_folder=raw_train_dir\n",
        "for i in range(3):\n",
        "    file = random.choice(os.listdir(img_folder))\n",
        "    image_path= os.path.join(img_folder, file)\n",
        "    img=mpimg.imread(image_path)\n",
        "    ax=plt.subplot(1,3,i+1)\n",
        "    ax.title.set_text(file)\n",
        "    plt.imshow(img,cmap='gray')\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "vBbGjzD-7cOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_folder=clean_train_dir\n",
        "for i in range(3):\n",
        "    file = random.choice(os.listdir(img_folder))\n",
        "    image_path= os.path.join(img_folder, file)\n",
        "    img=mpimg.imread(image_path)\n",
        "    ax=plt.subplot(1,3,i+1)\n",
        "    ax.title.set_text(file)\n",
        "    plt.imshow(img,cmap='gray')\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "Pz5mXCYU7e9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from sklearn.feature_extraction import image\n",
        "from PIL import Image\n",
        "\n",
        "def create_dataset(img_folder):\n",
        "    img_data_array=[]\n",
        "    path, dirs, files = next(os.walk(img_folder))\n",
        "    file_count = len(files)\n",
        "    file_names = sorted(os.listdir(img_folder))\n",
        "    print(file_count)\n",
        "    for i in range(file_count):\n",
        "        imgPath = img_folder+'/'+file_names[i]\n",
        "        img=Image.open(imgPath)\n",
        "        img=img.resize((128, 128),Image.LANCZOS)\n",
        "        img=np.array(img)\n",
        "        img = img.astype('float32')\n",
        "        img /= 255\n",
        "        img_data_array.append(img)\n",
        "    return img_data_array# extract the image array and class name"
      ],
      "metadata": {
        "id": "vTHdxkPU7gOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_data =create_dataset(raw_train_dir)\n",
        "clean_train_data=create_dataset(clean_train_dir)\n",
        "raw_test_data=create_dataset(raw_test_dir)\n",
        "clean_test_data=create_dataset(clean_test_dir)\n",
        "raw_val_data=create_dataset(raw_val_dir)\n",
        "clean_val_data=create_dataset(clean_val_dir)"
      ],
      "metadata": {
        "id": "15o8Uv2W7iOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_data = np.array(raw_train_data)\n",
        "clean_train_data = np.array(clean_train_data)\n",
        "raw_test_data = np.array(raw_test_data)\n",
        "clean_test_data = np.array(clean_test_data)\n",
        "raw_val_data = np.array(raw_val_data)\n",
        "clean_val_data = np.array(clean_val_data)"
      ],
      "metadata": {
        "id": "Mwrjf3gD7jvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_data.shape"
      ],
      "metadata": {
        "id": "7ZwXTNHh7lTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "TwaiqbKj7nGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to view image at a particular index\n",
        "def view_img_at_index(dataset, index):\n",
        "    plt.figure(figsize=(4, 6))\n",
        "    plt.imshow(dataset[index], cmap='gray')\n",
        "    plt.title(f\"Image at index {index}: \")\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "DrITCWtv7ogD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "i = random.randint(0, (len(raw_train_data)-1))\n",
        "i"
      ],
      "metadata": {
        "id": "C8CqQpKy7pyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_img_at_index(raw_train_data, i)"
      ],
      "metadata": {
        "id": "pUMzE_-07rGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_img_at_index(clean_train_data, i)"
      ],
      "metadata": {
        "id": "0cgu6fxm7tP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Convolutional Block\n",
        "class Cust_Conv(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size=(3,3)):\n",
        "        super(Cust_Conv, self).__init__()\n",
        "\n",
        "        self.conv = Conv2D(filters, kernel_size, padding='same')\n",
        "        self.bn = BatchNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv(inputs)\n",
        "        x = self.bn(x)\n",
        "\n",
        "        return Activation('relu')(x)"
      ],
      "metadata": {
        "id": "CmgB7gK-7tNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Blocks\n",
        "class res_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size=(3,3)):\n",
        "        super(res_layer, self).__init__()\n",
        "\n",
        "        self.conv1 = Conv2D(filters, kernel_size, padding='same')\n",
        "        self.bn1 = BatchNormalization()\n",
        "        self.rel1 = Activation('relu')\n",
        "\n",
        "        self.conv2 = Conv2D(filters, kernel_size, padding='same')\n",
        "        self.bn2 = BatchNormalization()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x)\n",
        "        x = self.rel1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x += inputs\n",
        "        return Activation('relu')(x)"
      ],
      "metadata": {
        "id": "K6tJ3TGg7tLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow Wavelets\n",
        "\n",
        "import tensorflow_wavelets.Layers.DWT as DWT\n",
        "import tensorflow_wavelets.Layers.DMWT as DMWT\n",
        "import tensorflow_wavelets.Layers.DTCWT as DTCWT\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Conv2D\n",
        "# from keras.layers import BatchNormalization\n",
        "# from keras.layers import Activation"
      ],
      "metadata": {
        "id": "du_NjrM57tIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DTCWT Layer\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow_wavelets.utils import filters\n",
        "from tensorflow_wavelets.utils.helpers import *\n",
        "from tensorflow_wavelets.utils.cast import *\n",
        "class DTCWT(layers.Layer):\n",
        "    \"\"\"\n",
        "    Durel Tree Complex Wavelet Transform\n",
        "    Input: level - tree-level (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, level=1, **kwargs):\n",
        "        super(DTCWT, self).__init__(**kwargs)\n",
        "\n",
        "        if level <= 1:\n",
        "            level = 1\n",
        "\n",
        "        self.level = int(level)\n",
        "        self.conv_type = \"SAME\"\n",
        "        self.border_padd = \"SYMMETRIC\"\n",
        "\n",
        "        # Faf - First analysis filter - for the first level\n",
        "        # Fsf - First synthesis filter\n",
        "        faf, fsf = filters.fs_farras()\n",
        "        af, sf = filters.duelfilt()\n",
        "\n",
        "        # convert to tensor\n",
        "        self.Faf = duel_filter_tf(faf)\n",
        "        self.Fsf = duel_filter_tf(fsf)\n",
        "        self.af = duel_filter_tf(af)\n",
        "        self.sf = duel_filter_tf(sf)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # repeat last channel if input channel bigger then 1\n",
        "        if input_shape[-1] > 1:\n",
        "            self.Faf = tf.repeat(self.Faf, input_shape[-1], axis=-1)\n",
        "            self.Fsf = tf.repeat(self.Fsf, input_shape[-1], axis=-1)\n",
        "            self.af = tf.repeat(self.af, input_shape[-1], axis=-1)\n",
        "            self.sf = tf.repeat(self.sf, input_shape[-1], axis=-1)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "\n",
        "        # normalize\n",
        "        x_norm = tf.math.divide(inputs, 2)\n",
        "\n",
        "        # 2 trees J+1 lists\n",
        "        w = [[[[], []] for _ in range(2)] for __ in range(self.level+1)]\n",
        "\n",
        "        # 2 trees - 2 filters ( first stage is using differnet filter\n",
        "        for m in range(2):\n",
        "            for n in range(2):\n",
        "                [lo, w[0][m][n]] = analysis_filter_bank2d(x_norm, self.Faf[m][0], self.Faf[m][1],\n",
        "                                                          self.Faf[n][0], self.Faf[n][1])\n",
        "                for j in range(1, self.level):\n",
        "                    [lo, w[j][m][n]] = analysis_filter_bank2d(lo, self.af[m][0], self.af[m][1],\n",
        "                                                              self.af[n][0], self.af[n][1])\n",
        "                w[self.level][m][n] = lo\n",
        "\n",
        "        # add and subtract for the complex\n",
        "        for j in range(self.level):\n",
        "            for m in range(3):\n",
        "\n",
        "                w[j][0][0][m], w[j][1][1][m] = add_sub(w[j][0][0][m], w[j][1][1][m])\n",
        "                w[j][0][1][m], w[j][1][0][m] = add_sub(w[j][0][1][m], w[j][1][0][m])\n",
        "\n",
        "        # concat into one big image\n",
        "        # different resolution as the tree is deeper\n",
        "        # TODO: How to split different resolutions into different channels\n",
        "        j = 1\n",
        "        w_c = w\n",
        "\n",
        "        for j in [x for x in range(1, self.level)][::-1]:\n",
        "\n",
        "            w_c[j][0][0] = tf.concat([tf.concat([w_c[j+1][0][0], w_c[j][0][0][0]], axis=2),\n",
        "                                      tf.concat([w_c[j][0][0][1], w_c[j][0][0][2]], axis=2)], axis=1)\n",
        "            w_c[j][0][1] = tf.concat([tf.concat([w_c[j+1][0][1], w_c[j][0][1][0]], axis=2),\n",
        "                                      tf.concat([w_c[j][0][1][1], w_c[j][0][1][2]], axis=2)], axis=1)\n",
        "            w_c[j][1][0] = tf.concat([tf.concat([w_c[j+1][1][0], w_c[j][1][0][0]], axis=2),\n",
        "                                      tf.concat([w_c[j][1][0][1], w_c[j][1][0][2]], axis=2)], axis=1)\n",
        "            w_c[j][1][1] = tf.concat([tf.concat([w_c[j+1][1][1], w_c[j][1][1][0]], axis=2),\n",
        "                                      tf.concat([w_c[j][1][1][1], w_c[j][1][1][2]], axis=2)], axis=1)\n",
        "\n",
        "        w_0 = tf.concat([tf.concat([w_c[j][0][0], w_c[0][0][0][0]], axis=2),\n",
        "                         tf.concat([w_c[0][0][0][1], w_c[0][0][0][2]], axis=2)], axis=1)\n",
        "        w_1 = tf.concat([tf.concat([w_c[j][0][1], w_c[0][0][1][0]], axis=2),\n",
        "                         tf.concat([w_c[0][0][1][1], w_c[0][0][1][2]], axis=2)], axis=1)\n",
        "        w_2 = tf.concat([tf.concat([w_c[j][1][0], w_c[0][1][0][0]], axis=2),\n",
        "                         tf.concat([w_c[0][1][0][1], w_c[0][1][0][2]], axis=2)], axis=1)\n",
        "        w_3 = tf.concat([tf.concat([w_c[j][1][1], w_c[0][1][1][0]], axis=2),\n",
        "                         tf.concat([w_c[0][1][1][1], w_c[0][1][1][2]], axis=2)], axis=1)\n",
        "\n",
        "        w_1234 = tf.concat([tf.concat([w_0, w_1], axis=2), tf.concat([w_2, w_3], axis=2)], axis=1)\n",
        "        def get_config(self):\n",
        "            config = super(DTCWT,self).get_config().copy()\n",
        "            config.update({})\n",
        "            return config\n",
        "\n",
        "        return w_1234\n",
        "\n",
        "\n",
        "class IDTCWT(layers.Layer):\n",
        "    \"\"\"\n",
        "    Inverse Duel Tree Complex Wavelet Transform\n",
        "    Input: level - tree-level (int)\n",
        "    \"\"\"\n",
        "    def __init__(self, level=1, **kwargs):\n",
        "        super(IDTCWT, self).__init__(**kwargs)\n",
        "\n",
        "        if level <= 1:\n",
        "            level = 1\n",
        "\n",
        "        self.level = int(level)\n",
        "        self.conv_type = \"SAME\"\n",
        "        self.border_padd = \"SYMMETRIC\"\n",
        "\n",
        "        # Faf - First analysis filter - for the first level\n",
        "        # Fsf - First synthesis filter\n",
        "        faf, fsf = filters.fs_farras()\n",
        "        af, sf = filters.duelfilt()\n",
        "\n",
        "        self.Faf = duel_filter_tf(faf)\n",
        "        self.Fsf = duel_filter_tf(fsf)\n",
        "        self.af = duel_filter_tf(af)\n",
        "        self.sf = duel_filter_tf(sf)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # repeat last channel if input channel bigger then 1\n",
        "        if input_shape[-1] > 1:\n",
        "            self.Faf = tf.repeat(self.Faf, input_shape[-1], axis=-1)\n",
        "            self.Fsf = tf.repeat(self.Fsf, input_shape[-1], axis=-1)\n",
        "            self.af = tf.repeat(self.af, input_shape[-1], axis=-1)\n",
        "            self.sf = tf.repeat(self.sf, input_shape[-1], axis=-1)\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "\n",
        "        # convert one big image into list of tree levels\n",
        "        w_rec = reconstruct_w_leveln(inputs, self.level)\n",
        "\n",
        "        height = int(w_rec[0][0][0][0].shape[1]*2)\n",
        "        width = int(w_rec[0][0][0][0].shape[2]*2)\n",
        "\n",
        "        # init image to be reconstructed\n",
        "        y = tf.zeros((height, width, inputs.shape[-1]), dtype=tf.float32)\n",
        "\n",
        "        w_i = [[[[list() for _ in range(3)], [list() for _ in range(3)]]\n",
        "                for __ in range(2)] for ___ in range(self.level+1)]\n",
        "\n",
        "        # first add and subtract (inverse the transform)\n",
        "        for j in range(self.level):\n",
        "            for m in range(3):\n",
        "\n",
        "                w_i[j][0][0][m], w_i[j][1][1][m] = add_sub(w_rec[j][0][0][m], w_rec[j][1][1][m])\n",
        "                w_i[j][0][1][m], w_i[j][1][0][m] = add_sub(w_rec[j][0][1][m], w_rec[j][1][0][m])\n",
        "\n",
        "        # synthesis with the First filters to be last in the reconstruction\n",
        "        for m in range(2):\n",
        "            for n in range(2):\n",
        "                lo = w_rec[self.level][m][n]\n",
        "                for j in [x for x in range(1, self.level)][::-1]:\n",
        "                    lo = synthesis_filter_bank2d(lo, w_i[j][m][n], self.sf[m][0],\n",
        "                                                 self.sf[m][1], self.sf[n][0], self.sf[n][1])\n",
        "                lo = synthesis_filter_bank2d(lo, w_i[0][m][n], self.Fsf[m][0],\n",
        "                                             self.Fsf[m][1], self.Fsf[n][0], self.Fsf[n][1])\n",
        "                y = tf.math.add(y, lo)\n",
        "\n",
        "        # revert the normalization\n",
        "        y = tf.math.divide(y, 2)\n",
        "        def get_config(self):\n",
        "                config = super(IDTCWT,self).get_config().copy()\n",
        "                config.update({})\n",
        "                return config\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ],
      "metadata": {
        "id": "7ZH2hj4V7tF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow_wavelets.Layers.DWT as DWT\n",
        "# import tensorflow_wavelets.Layers.DTCWT as DTCWT\n",
        "# import tensorflow_wavelets.Layers.DMWT as DMWT\n",
        "inputs = Input(shape = (128,128,1))\n",
        "waveTrans1=DTCWT(level=3)(inputs)\n",
        "# waveTrans1=DMWT.DMWT(wavelet_name='ghm')(inputs)\n",
        "# waveTrans1=DWTPooling(wavelet=\"db1\")(inputs)\n",
        "out0 = Cust_Conv(80, (3,3))(waveTrans1)\n",
        "out1 = Cust_Conv(80, (3,3))(out0)\n",
        "out2 = Cust_Conv(80, (3,3))(out1)\n",
        "out3 = Cust_Conv(80, (3,3))(out2)\n",
        "out3.shape"
      ],
      "metadata": {
        "id": "Wvf2XE1T7tDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveTrans1.shape"
      ],
      "metadata": {
        "id": "vDULycJs7tBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out4 = Cust_Conv(160, (3,3))(out3)\n",
        "out5 = Cust_Conv(160, (3,3))(out4)\n",
        "out6 = Cust_Conv(160, (3,3))(out5)\n",
        "out7 = Cust_Conv(160, (3,3))(out6)\n",
        "out7.shape"
      ],
      "metadata": {
        "id": "J-OomEft7s-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out8 = res_layer(160, (3,3))(out7)\n",
        "out9 = res_layer(160, (3,3))(out8)\n",
        "out10 = res_layer(160, (3,3))(out9)\n",
        "out11 = res_layer(160, (3,3))(out10)\n",
        "out12 = res_layer(160, (3,3))(out11)\n",
        "out13 = res_layer(160, (3,3))(out12)\n",
        "out14 = res_layer(160, (3,3))(out13)\n",
        "out14.shape"
      ],
      "metadata": {
        "id": "MEPpTFIA7-QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveadd1 = Add()([out14, out7])\n",
        "# waveTrans4=IDTCWT(level=3)(waveadd1)\n",
        "# print(waveTrans4.shape)\n",
        "out15 = Cust_Conv(160, (3,3))(waveadd1)\n",
        "out16 = Cust_Conv(160, (3,3))(out15)\n",
        "out17 = Cust_Conv(160, (3,3))(out16)\n",
        "out18 = Cust_Conv(160, (3,3))(out17)\n",
        "out19 = Cust_Conv(80, (3,3))(out18)\n",
        "out19.shape"
      ],
      "metadata": {
        "id": "nmT-mGdm7-Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waveadd2 = Add()([out19, out3])\n",
        "out20 = Cust_Conv(80, (3,3))(out19)\n",
        "out21 = Cust_Conv(80, (3,3))(out20)\n",
        "out22 = Cust_Conv(80, (3,3))(out21)\n",
        "out23 = Cust_Conv(80, (3,3))(out22)\n",
        "out23.shape"
      ],
      "metadata": {
        "id": "bFVzsFSU7-LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = Cust_Conv(filters=1, kernel_size=(1,1))(out23)\n",
        "output = Add()([output, waveTrans1])\n",
        "output = IDTCWT(level=3)(output)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "LCtwgCCE7-GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Model(inputs = inputs, outputs = output)"
      ],
      "metadata": {
        "id": "honRZ6z97-Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "              loss = 'mean_squared_error',\n",
        "              optimizer = tf.keras.optimizers.Adam(lr=10e-4, beta_1=0.9, beta_2=0.999, epsilon=10e-8, decay=10e-3)\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NkjRUp1V7-A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=8\n",
        "checkpoint_path = \"D:/SHREYASH_CODE/weights/MWRCNN/checkpoint-{epoch:00d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "# Create a callback that saves the model's weights every epoch\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_freq=9*batch_size)"
      ],
      "metadata": {
        "id": "iWxWciXc79-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(raw_train_data,clean_train_data,batch_size=batch_size,epochs=25,callbacks=[cp_callback],validation_data=(raw_val_data,clean_val_data))"
      ],
      "metadata": {
        "id": "LxF2kbFO797p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "metadata": {
        "id": "QHdgLWyy795B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(latest)"
      ],
      "metadata": {
        "id": "PMpUF7ud792Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out=model.predict(raw_test_data)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "UYetBLBk7s7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_data.shape, clean_test_data.shape"
      ],
      "metadata": {
        "id": "nz0-0iit7s36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0, (len(raw_test_data)-1))\n",
        "i"
      ],
      "metadata": {
        "id": "s4PV3DAY7s1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(out[i],(128, 128)),cmap='gray')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "g1dHKGfq8TX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(clean_test_data[i],(128, 128)),cmap='gray')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "3mlbdbcV8TWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(raw_test_data[i],(128, 128)),cmap='gray')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "GY-dIy-H8TTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum1 = 0\n",
        "sum2 = 0\n",
        "\n",
        "from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, structural_similarity as ssim\n",
        "for i in range(len(raw_test_data)):\n",
        "    outImg=np.reshape(out[i],(128,128))\n",
        "    sum1 = sum1+peak_signal_noise_ratio(clean_test_data[i],outImg)\n",
        "    sum2 = sum2+ssim(clean_test_data[i],outImg)\n",
        "\n",
        "sum1, sum2"
      ],
      "metadata": {
        "id": "tPv0ksta8TQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psnr_avg = sum1/len(raw_test_data)\n",
        "ssim_avg = sum2/len(raw_test_data)\n",
        "\n",
        "print(\"Average PSNR: \", psnr_avg)\n",
        "print(\"Average SSIM: \", ssim_avg)"
      ],
      "metadata": {
        "id": "2JUJN7My8TOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_original_dataset(img_folder):\n",
        "    img_data_array=[]\n",
        "    path, dirs, files = next(os.walk(img_folder))\n",
        "    file_count = len(files)\n",
        "    file_names = sorted(os.listdir(img_folder))\n",
        "    print(file_count)\n",
        "    for i in range(file_count):\n",
        "        imgPath = img_folder+'/'+file_names[i]\n",
        "        image= cv2.imread( imgPath, cv2.IMREAD_GRAYSCALE)\n",
        "        image=np.array(image)\n",
        "        image = image.astype('float32')\n",
        "        image /= 255\n",
        "        img_data_array.append(image)\n",
        "    return img_data_array# extract the image array and class name"
      ],
      "metadata": {
        "id": "YFAEP4KY8TLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ori_clean = create_original_dataset(clean_test_dir)\n",
        "ori_clean = np.array(ori_clean)\n",
        "ori_raw = create_original_dataset(raw_test_dir)\n",
        "ori_raw = np.array(ori_raw)"
      ],
      "metadata": {
        "id": "MTUQIaYl8TJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(raw_test_data)):\n",
        "    print(ori_clean[i].shape, ori_raw[i].shape)"
      ],
      "metadata": {
        "id": "YtWXNSpQ8TGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ori_out = []\n",
        "for i in range(len(raw_test_data)):\n",
        "    image = out[i]\n",
        "    img1 = cv2.resize(image, (900, 450), interpolation = cv2.INTER_AREA)\n",
        "    img1 = np.array(img1)\n",
        "    ori_out.append(img1)\n",
        "\n",
        "ori_out = np.array(ori_out)\n",
        "ori_out.shape"
      ],
      "metadata": {
        "id": "DIPeQLhb8gmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = random.randint(0, (len(raw_test_data)-1))\n",
        "i"
      ],
      "metadata": {
        "id": "Bg__sxZ48gj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(ori_out[i],(450, 900)),cmap='gray')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "W0Zp83Tr8ghL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(ori_clean[i],(450, 900)),cmap='gray')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "fry5b8sW8mTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.reshape(ori_raw[i],(450, 900)),cmap='gray')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "CScEo6lE8mQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peak_signal_noise_ratio(ori_clean[i], ori_out[i]), ssim(ori_clean[i], ori_out[i])"
      ],
      "metadata": {
        "id": "Og1-y-wA8mOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum3 = 0\n",
        "sum4 = 0\n",
        "\n",
        "from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, structural_similarity as ssim\n",
        "for i in range(len(raw_test_data)):\n",
        "    outImg=ori_out[i]\n",
        "    sum3 = sum3+peak_signal_noise_ratio(ori_clean[i],outImg)\n",
        "    sum4 = sum4+ssim(ori_clean[i],outImg)\n",
        "\n",
        "sum3, sum4"
      ],
      "metadata": {
        "id": "KDqlVpoa8mL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "psnr_avg_ori = sum3/len(raw_test_data)\n",
        "ssim_avg_ori = sum4/len(raw_test_data)\n",
        "\n",
        "print(\"Average PSNR on Original Size: \", psnr_avg_ori)\n",
        "print(\"Average SSIM on Original Size: \", ssim_avg_ori)"
      ],
      "metadata": {
        "id": "vRwe2qhg8mJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = \"D:/SHREYASH_CODE/OUTPUTS/MWRDCNN_DTCWT\""
      ],
      "metadata": {
        "id": "ivMTATDp8tNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(ori_out)):\n",
        "    img = ori_out[i]\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(f\"{out_dir}/out_{str(i)}.tif\")"
      ],
      "metadata": {
        "id": "leCGq_3d8tLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}